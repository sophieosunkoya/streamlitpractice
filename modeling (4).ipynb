{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "817caad8",
   "metadata": {},
   "source": [
    "#  Student Performance Prediction\n",
    "\n",
    "This notebook develops regression models to predict student exam scores using lifestyle, health, and background variables.\n",
    "We use four models: Linear Regression, Random Forest, Gradient Boosting, and K-Nearest Neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7cd157",
   "metadata": {},
   "source": [
    "## Project Objective and Overarching Question\n",
    "The central question driving this project is:\n",
    "**To what extent can student exam scores be predicted from lifestyle habits, wellness factors, and socioeconomic background?**\n",
    "\n",
    "We aim to identify which features contribute most to academic performance and explore predictive models that can help estimate student outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36754ce0",
   "metadata": {},
   "source": [
    "#  Student Performance Prediction\n",
    "\n",
    "This notebook develops regression models to predict student exam scores using lifestyle, health, and background variables.\n",
    "We use four models: Linear Regression, Random Forest, Gradient Boosting, and K-Nearest Neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaac935",
   "metadata": {},
   "source": [
    "##  Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8c83df",
   "metadata": {},
   "source": [
    "We import all required libraries including models, pipelines, and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d08c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "df = pd.read_csv('../../student_habits_performance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c43134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop ID and separate features/target\n",
    "X = df.drop(columns=['student_id', 'exam_score'])\n",
    "y = df['exam_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e040574",
   "metadata": {},
   "source": [
    "We define the target (exam_score) and drop non-useful columns like student_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2a304c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate categorical and numeric features\n",
    "categorical_cols = X.select_dtypes(include='object').columns.tolist()\n",
    "numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a06426",
   "metadata": {},
   "source": [
    "##  Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbe42ca",
   "metadata": {},
   "source": [
    "Here we separate numeric and categorical features to process them differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b09fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), numeric_cols),\n",
    "    ('cat', OneHotEncoder(drop='first'), categorical_cols)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fcfecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873b3388",
   "metadata": {},
   "source": [
    "This preprocessing step scales numeric values and one-hot encodes categoricals to prepare them for machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59278f23",
   "metadata": {},
   "source": [
    "##  Models Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aba5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n",
    "    'K-Nearest Neighbors': KNeighborsRegressor(n_neighbors=5)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23393e59",
   "metadata": {},
   "source": [
    "Train-test split ensures we evaluate models on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ff3080",
   "metadata": {},
   "source": [
    "##  Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7188a73d",
   "metadata": {},
   "source": [
    "We define four regression models to compare performance using familiar techniques from class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13dc479",
   "metadata": {},
   "source": [
    "### Interpretation of Modeling Results\n",
    "The table summarizes each model's predictive performance:\n",
    "- **Random Forest** and **Gradient Boosting** had the highest R² scores, indicating better generalization and predictive accuracy.\n",
    "- **Linear Regression** performed reasonably well, showing that there is a linear relationship between some variables and scores.\n",
    "- **K-Nearest Neighbors** had the lowest R² and highest error, likely because it's sensitive to feature scaling and local variation.\n",
    "\n",
    "These results suggest that tree-based models handle this mix of features well, especially when nonlinear patterns exist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969772e3",
   "metadata": {},
   "source": [
    "## Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f93c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "lr_pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "lr_pipe.fit(X_train, y_train)\n",
    "lr_preds = lr_pipe.predict(X_test)\n",
    "print(\"Linear Regression R²:\", r2_score(y_test, lr_preds))\n",
    "print(\"RMSE:\", mean_squared_error(y_test, lr_preds, squared=False))\n",
    "print(\"MAE:\", mean_absolute_error(y_test, lr_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803a3177",
   "metadata": {},
   "source": [
    "## Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1f791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Regressor\n",
    "rf_pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestRegressor(random_state=42))\n",
    "])\n",
    "rf_pipe.fit(X_train, y_train)\n",
    "rf_preds = rf_pipe.predict(X_test)\n",
    "print(\"Random Forest R²:\", r2_score(y_test, rf_preds))\n",
    "print(\"RMSE:\", mean_squared_error(y_test, rf_preds, squared=False))\n",
    "print(\"MAE:\", mean_absolute_error(y_test, rf_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce48110",
   "metadata": {},
   "source": [
    "## Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf683953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Regressor\n",
    "gb_pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', GradientBoostingRegressor(random_state=42))\n",
    "])\n",
    "gb_pipe.fit(X_train, y_train)\n",
    "gb_preds = gb_pipe.predict(X_test)\n",
    "print(\"Gradient Boosting R²:\", r2_score(y_test, gb_preds))\n",
    "print(\"RMSE:\", mean_squared_error(y_test, gb_preds, squared=False))\n",
    "print(\"MAE:\", mean_absolute_error(y_test, gb_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28c8c07",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35123b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neighbors Regressor\n",
    "knn_pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', KNeighborsRegressor(n_neighbors=5))\n",
    "])\n",
    "knn_pipe.fit(X_train, y_train)\n",
    "knn_preds = knn_pipe.predict(X_test)\n",
    "print(\"KNN R²:\", r2_score(y_test, knn_preds))\n",
    "print(\"RMSE:\", mean_squared_error(y_test, knn_preds, squared=False))\n",
    "print(\"MAE:\", mean_absolute_error(y_test, knn_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3e5bd7",
   "metadata": {},
   "source": [
    "\n",
    "## Final Interpretation and Key Takeaways\n",
    "\n",
    "The Random Forest and Gradient Boosting models performed the best in our evaluation. Their ability to capture non-linear patterns and handle mixed data types made them ideal for this dataset.\n",
    "\n",
    "Most influential features across models were:\n",
    "- Study hours per day\n",
    "- Class attendance\n",
    "- Mental health rating\n",
    "- Sleep hours\n",
    "- Diet and exercise frequency\n",
    "\n",
    "This confirms that academic performance isn't just about studying longer—wellness and environmental factors matter significantly. These results can help educators support students holistically.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
